{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdA4KjrNwn5B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ELE6310 - Assignment 1 - Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm4CU_e6xHpL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Name: \n",
    "#### Student ID: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGmDZ11LULkF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Mount your Google Drive\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DF9zCVJULkH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Link your assignment folder & install requirements\n",
    "#@markdown Enter the path to the assignment folder in your Google Drive\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "folder = \"/content/gdrive/MyDrive/ELE6310/A1\" #@param {type:\"string\"}\n",
    "!ln -Ts $folder /content/A1 2> /dev/null\n",
    "\n",
    "# Add the assignment folder to Python path\n",
    "if '/content/A1' not in sys.path:\n",
    "    sys.path.insert(0, '/content/A1')\n",
    "\n",
    "# Install requirements\n",
    "!pip install -qr /content/A1/requirements.txt\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnYxJmVqyj6a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1- Calibration [50 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcJ0NuyOoZ4p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import solution\n",
    "from common.test_functions import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R6Y-rcbVuMG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* First, complete `linear_quantize`, `linear_dequantize`, `update_scale_and_zero_point`, and `get_scale` functions  in `solution.py`and then run the below tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usu5pr3cVUX0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_linear_quantize()\n",
    "test_linear_dequantize()\n",
    "test_update_scale_and_zero_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moxo1oHfWVXk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Now we will see the performance of each quantization method on there different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHHh5KEolgvc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = torch.load(os.path.join(folder,'Dataset_A.t')) \n",
    "plot_real_dequantized_histogram(data, N_bits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxPGGyqJlgvd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = torch.load(os.path.join(folder,'Dataset_B.t'))\n",
    "plot_real_dequantized_histogram(data, N_bits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaDHpDHQ3bWG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compare your results. Which method works better? Do you think the quantiztion error has a bias? explain your observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnK3c81SknhI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{array}{|c|ccc|ccc|}\\hline\\\\ \n",
    "     Dataset && A &&& B \\\\ \\hline\n",
    "Bit width & 8 & 4 & 2 & 8 & 4 & 2 \\\\ \\hline\n",
    "Symmetric & ?? & ?? & ?? & ?? & ?? & ?? \\\\ \n",
    "Heuristic Method & ?? & ?? & ?? & ?? & ?? & ?? \\\\ \n",
    "SAWB & ?? & ?? & ?? & ?? & ?? & ?? \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpegJgT9RVvG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2- PTQ -vs- QTA [60 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUDro-oYXJL3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Complete `quantize_func_STE` in order to the quantization block (linear quantize and dequntize together) to meet the STE condition.\n",
    "\n",
    "\n",
    "* Complete `quantized_linear_function` and `quantized_conv2d_function` function only using `integer_linear` and `integer_conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_STE_grad()"
   ],
   "metadata": {
    "id": "FOdnoeBamS0w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NfwlhUbqlgve"
   },
   "outputs": [],
   "source": [
    "test_quantized_linear_function(weight_N_bits=2, act_N_bits=8, method='SAWB', bias=False)\n",
    "test_quantized_linear_module(weight_N_bits=8, act_N_bits=8, method='sym', bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "23nKTFn-lgve"
   },
   "outputs": [],
   "source": [
    "test_quantized_conv2d_function(weight_N_bits=2, act_N_bits=2, method='sym', bias=False)\n",
    "test_quantized_conv2d_module(weight_N_bits=8, act_N_bits=8, method='SAWB', bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31MbpcH2S7cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* In this assignment we use resnet20 with pre-trained weights on CIFAR10. First, Let's see the accuracy and model size of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HYspOWxw29A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from common.utils import load_CIFAR10_dataset, evaluate, fit, model_size\n",
    "from common.resnet import resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGCwFFPmlgve",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Seed = 6310\n",
    "torch.manual_seed(Seed)\n",
    "np.random.seed(Seed)\n",
    "random.seed(Seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Seed)\n",
    "    torch.cuda.manual_seed_all(Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqkaY-CWaXR3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, calibration_loader = load_CIFAR10_dataset(batch_size=256, calibration_batch_size=1024)\n",
    "model = resnet20(pretrained=True, save_path='./save/')\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "\n",
    "accuracy = evaluate(model, test_loader, device)\n",
    "print(\"test accuracy of fp model:\", accuracy)\n",
    "model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N36zX5HacnPA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* In the first step, we use calibration set to initial scale factors of the activation in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhAc13r8ckCc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "method='sym'\n",
    "act_N_bits=4\n",
    "weight_N_bits=4\n",
    "quantized_model = model_to_quant(model, calibration_loader, act_N_bits, weight_N_bits,method, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xJKTr4Ilgvf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate(quantized_model, test_loader, device)\n",
    "print(\"test accuracy of fp model:\", accuracy)\n",
    "model_size(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9xPNaD_lgvf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_layers_histogram(quantized_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH4Cx3aVrufE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Try `W8A8`, `W4A4`, `W2A2`, `W8A2`, and `W2A8` quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Now try to fine-tune the specified models using any desired training method, and save the best performing model"
   ],
   "metadata": {
    "id": "jlIvyJ_locDq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(quantized_model.parameters(), 1e-4, momentum=0.9, weight_decay=0.0005, nesterov=True)\n",
    "scheduler = None\n",
    "\n",
    "train_accuracy, test_accuracy = fit(quantized_model, 5, train_loader, test_loader, criterion, optimizer, scheduler, device)\n"
   ],
   "metadata": {
    "id": "4ksx4EKcoYqh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCF0o0HMpDob",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3- Variable precision [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5ZoLeeFXUSm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the  Variable precision (or \"Mixed-precision\") method, each layer is quantized with different bit precision. In this part, we want to find the optimal model-size for the resnet20 on the CIFAR-10 dataset. For this part, we only focus on weight quantization (with signed symmetric method) and we keep the activation in 16 bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCUcFhgVU3vE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Use a method of your choice to find the optimal model size with a constraint on test accuracy above 85\\%. \n",
    "Any reasonable attempt at exploring the design space will give you full marks. Better approaches/results will be considered for bonus points. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{array}{|c|cc|cc|}\\hline\\\\ \n",
    "      & PTQ && QAT \\\\ \\hline\n",
    "method & Symmetric & SAWB & Symmetric & SAWB \\\\ \\hline\n",
    "W8A8 & ?? & ?? & ?? & ?? \\\\ \n",
    "W4A4 & ?? & ?? & ?? & ?? \\\\ \n",
    "W2A2 & ?? & ?? & ?? & ?? \\\\ \n",
    "W8A2 & ?? & ?? & ?? & ?? \\\\ \n",
    "W2A8 & ?? & ?? & ?? & ?? \\\\ \\hline\n",
    "\\end{array}"
   ],
   "metadata": {
    "id": "IlXvpO7C_px5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LObcz_mlgvf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bitwidth_dict = {\n",
    "    'layer1.0.conv1': 8,\n",
    "    'layer1.0.conv2': 8,\n",
    "    'layer1.1.conv1': 8,\n",
    "    'layer1.1.conv2': 8,\n",
    "    'layer1.2.conv1': 8,\n",
    "    'layer1.2.conv2': 8,\n",
    "    \n",
    "    'layer2.0.conv1': 8,\n",
    "    'layer2.0.conv2': 8,\n",
    "    'layer2.0.downsample.0': 8,\n",
    "    'layer2.1.conv1': 8,\n",
    "    'layer2.1.conv2': 8,\n",
    "    'layer2.2.conv1': 8,\n",
    "    'layer2.2.conv2': 8,\n",
    "    \n",
    "    'layer3.0.conv1': 8,\n",
    "    'layer3.0.conv2': 8,\n",
    "    'layer3.0.downsample.0': 8,\n",
    "    'layer3.1.conv1': 8,\n",
    "    'layer3.1.conv2': 8,\n",
    "    'layer3.2.conv1': 8,\n",
    "    'layer3.2.conv2': 8,\n",
    "    \n",
    "    'fc': 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_DTblJwlgvf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "method='sym'\n",
    "act_N_bits=16\n",
    "quantized_model = model_to_quant(model, calibration_loader, act_N_bits, weight_N_bits,method, q_method, device, bitwidth_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfoimdbXlgvg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate(quantized_model, test_loader, device)\n",
    "print(\"test accuracy of fp model:\", accuracy)\n",
    "model_size(quantized_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}